{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Building a basic domain specific question and answer chat application(RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Notebook content\n",
    "This notebook contains the steps and code to demonstrate Retrieval Augumented Generation to build a chat application to answer questions specific to a domain. It introduces commands for data retrieval, knowledge base building & querying, and model testing.\n",
    "\n",
    "### About Retrieval Augmented Generation\n",
    "Retrieval Augmented Generation (RAG) is a versatile pattern that can unlock a number of use cases requiring factual recall of information, such as querying a knowledge base in natural language.\n",
    "\n",
    "In its simplest form, RAG requires 3 steps:\n",
    "\n",
    "- Index knowledge base passages (once)\n",
    "- Retrieve relevant passage(s) from knowledge base (for every user query)\n",
    "- Generate a response by feeding retrieved passage into a large language model (for every user query)\n",
    "\n",
    "## Contents\n",
    "\n",
    "This notebook contains the following parts:\n",
    "\n",
    "- [Introduction to RAG](#intro)\n",
    "- [Setup](#setup)\n",
    "- [Document data loading](#data)\n",
    "- [Build up knowledge base](#build_base)\n",
    "- [Foundation Models on watsonx](#models)\n",
    "- [Generate a retrieval-augmented response to a question](#predict)\n",
    "- [References](#references)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"intro\"></a>\n",
    "## Introduction to Retrieval Augmented Generation(RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG implementation flow:\n",
    "![image](https://dataplatform.cloud.ibm.com/docs/api/content/wsj/analyze-data/images/fm-rag-embed.svg?context=wx&locale=en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"setup\"></a>\n",
    "##  Set up the environment\n",
    "\n",
    "- Create API key from platform.openai.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Install and import the dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install \"langchain==0.1.10\" | tail -n 1\n",
    "!pip install \"ibm-watsonx-ai>=0.2.6\" | tail -n 1\n",
    "!pip install -U langchain_ibm | tail -n 1\n",
    "!pip install wget | tail -n 1\n",
    "!pip install sentence-transformers | tail -n 1\n",
    "!pip install \"chromadb==0.3.26\" | tail -n 1\n",
    "!pip install \"pydantic==1.10.0\" | tail -n 1\n",
    "!pip install \"sqlalchemy==2.0.1\" | tail -n 1\n",
    "!pip install \"pypdf\" | tail -n 1\n",
    "!pip install \"openai\" | tail -n 1\n",
    "!pip install \"tiktoken\" | tail -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os, getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### OpenAI API connection\n",
    "This cell defines the credentials required to work with Model inferencing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "openai_apikey = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"data\"></a>\n",
    "## Document data loading\n",
    "\n",
    "Download the file with State of the Union."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded itr-faq.pdf\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "filename = 'itr-faq.pdf'\n",
    "url = 'https://www.incometax.gov.in/iec/foportal/sites/default/files/2024-06/Top%2010%20issues%20of%20taxpayers%20updated.pdf'\n",
    "\n",
    "if not os.path.isfile(filename):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded {filename}\")\n",
    "    else:\n",
    "        print(f\"Failed to download file. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 krishna  staff  231539 Jul 27 08:05 itr-faq.pdf\n"
     ]
    }
   ],
   "source": [
    "ls -ltr itr-faq*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"build_base\"></a>\n",
    "## Build up knowledge base\n",
    "\n",
    "The most common approach in RAG is to create dense vector representations of the knowledge base in order to calculate the semantic similarity to a given user query.\n",
    "\n",
    "In this basic example, we take the Income tax returns FAQ content (in PDF file), split it into chunks, embed it using an open-source embedding model, load it into <a href=\"https://www.trychroma.com/\" target=\"_blank\" rel=\"noopener no referrer\">Chroma</a>, and then query it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "loader = PyPDFLoader(filename)\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The dataset we are using is already split into self-contained passages that can be ingested by Chroma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create an embedding function\n",
    "\n",
    "Note that you can feed a custom embedding function to be used by chromadb. The performance of Chroma db may differ depending on the embedding model used. In following example we use watsonx.ai Embedding service. We can check available embedding models using `get_embedding_model_specs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_apikey\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "docsearch = Chroma.from_documents(texts, embeddings, collection_name=\"itr-docs-openai\", persist_directory= \"/tmp/vector_data_oai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain CustomLLM wrapper for Openai model\n",
    "Initialize the `OpenAI` class from Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm_openai = OpenAI(temperature=0.7, max_tokens=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"predict\"></a>\n",
    "## Generate a retrieval-augmented response to a question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the `RetrievalQA` (question answering chain) to automate the RAG task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm_openai, chain_type=\"stuff\", retriever=docsearch.as_retriever())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Select questions\n",
    "\n",
    "Get questions from the previously loaded test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "  how to select a bank for refund\n",
      "Answer:\n",
      "  To select a bank for refund, the user needs to add a bank account in which they would like to receive the refund. This can be done by going to Profile>> My Bank Account and adding the necessary details. The request will then be sent to the respective bank or NPCI for validation. Once the validation is successful, the taxpayer can nominate the bank account for the refund.\n"
     ]
    }
   ],
   "source": [
    "query = \"how to select a bank for refund\"\n",
    "print(f\"Question:\\n  {query}\")\n",
    "answer = qa.invoke(query)[\"result\"]\n",
    "print(f\"Answer:\\n {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "  What are the documentes required to register for legal heir\n",
      "Answer:\n",
      " \n",
      "The documents required to register as a legal heir may vary depending on the specific circumstances, but generally they include:\n",
      "\n",
      "1. Copy of the deceased person's PAN card.\n",
      "2. Copy of the legal heir certificate issued by a court of law or local revenue authorities.\n",
      "3. Scanned copy of the PAN card of the legal heir.\n",
      "4. Scanned PDF copy of identity and address proof of the legal heir (such as passport, voter ID, driving license, Aadhaar card, or bank passbook with photo).\n",
      "5. Letter in writing requesting to register as a legal heir.\n",
      "6. Scanned copy of the death certificate issued by the Municipal Authority, Corporation, or Registrar of Deaths.\n",
      "7. Copy of any order passed in the name of the deceased, if applicable.\n",
      "8. Copy of a letter of indemnity (optional).\n",
      "9. In case any document is in a vernacular language, a Hindi/English translation of the document duly notarized, along with a copy of the\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the documentes required to register for legal heir\"\n",
    "print(f\"Question:\\n  {query}\")\n",
    "answer = qa.invoke(query)[\"result\"]\n",
    "print(f\"Answer:\\n {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "  Hello\n",
      "Answer:\n",
      "  I'm sorry, I am only able to assist with questions related to income tax filing. How can I help you with that?\n"
     ]
    }
   ],
   "source": [
    "query = \"Hello\"\n",
    "print(f\"Question:\\n  {query}\")\n",
    "answer = qa.invoke(query)[\"result\"]\n",
    "print(f\"Answer:\\n {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"references\"></a>\n",
    "## References\n",
    "- [Using vectorized text with retrieval-augmented generation tasks](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-embedding-rag.html?context=wx&audience=wdp)\n",
    "\n",
    "- [Sample notebook](https://dataplatform.cloud.ibm.com/exchange/public/entry/view/d3a5f957-a93b-46cd-82c1-c8d37d4f62c6?context=wx?context=wx&audience=wdp)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
