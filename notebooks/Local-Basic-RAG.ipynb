{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Building a basic domain specific question and answer chat application(RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Notebook content\n",
    "This notebook contains the steps and code to demonstrate Retrieval Augumented Generation to build a chat application to answer questions specific to a domain. It introduces commands for data retrieval, knowledge base building & querying, and model testing.\n",
    "\n",
    "### About Retrieval Augmented Generation\n",
    "Retrieval Augmented Generation (RAG) is a versatile pattern that can unlock a number of use cases requiring factual recall of information, such as querying a knowledge base in natural language.\n",
    "\n",
    "In its simplest form, RAG requires 3 steps:\n",
    "\n",
    "- Index knowledge base passages (once)\n",
    "- Retrieve relevant passage(s) from knowledge base (for every user query)\n",
    "- Generate a response by feeding retrieved passage into a large language model (for every user query)\n",
    "\n",
    "## Contents\n",
    "\n",
    "This notebook contains the following parts:\n",
    "\n",
    "- [Introduction to RAG](#intro)\n",
    "- [Setup](#setup)\n",
    "- [Document data loading](#data)\n",
    "- [Build up knowledge base](#build_base)\n",
    "- [Foundation Models on watsonx](#models)\n",
    "- [Generate a retrieval-augmented response to a question](#predict)\n",
    "- [References](#references)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"intro\"></a>\n",
    "## Introduction to Retrieval Augmented Generation(RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG implementation flow:\n",
    "![image](https://dataplatform.cloud.ibm.com/docs/api/content/wsj/analyze-data/images/fm-rag-embed.svg?context=wx&locale=en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"setup\"></a>\n",
    "##  Set up the environment\n",
    "\n",
    "Before you use the sample code in this notebook, you must perform the following setup tasks:\n",
    "\n",
    "-  Create a <a href=\"https://cloud.ibm.com/catalog/services/watson-machine-learning\" target=\"_blank\" rel=\"noopener no referrer\">Watson Machine Learning (WML) Service</a> instance (a free plan is offered and information about how to create the instance can be found <a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/wml-plans.html?context=wx&audience=wdp\" target=\"_blank\" rel=\"noopener no referrer\">here</a>).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Install and import the dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install \"langchain==0.1.10\" | tail -n 1\n",
    "!pip install \"ibm-watsonx-ai>=0.2.6\" | tail -n 1\n",
    "!pip install -U langchain_ibm | tail -n 1\n",
    "!pip install wget | tail -n 1\n",
    "!pip install sentence-transformers | tail -n 1\n",
    "!pip install \"chromadb==0.3.26\" | tail -n 1\n",
    "!pip install \"pydantic==1.10.0\" | tail -n 1\n",
    "!pip install \"sqlalchemy==2.0.1\" | tail -n 1\n",
    "!pip install \"pypdf\" | tail -n 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os, getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### watsonx API connection\n",
    "This cell defines the credentials required to work with watsonx API for Foundation\n",
    "Model inferencing.\n",
    "\n",
    "**Action:** Provide the IBM Cloud user API key. For details, see <a href=\"https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui\" target=\"_blank\" rel=\"noopener no referrer\">documentation</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "credentials = {\n",
    "    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "    \"apikey\": \"****\" \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Defining the project id\n",
    "The API requires project id that provides the context for the call. We will obtain the id from the project in which this notebook runs. Otherwise, please provide the project id.\n",
    "\n",
    "**Hint**: You can find the `project_id` as follows. Open the prompt lab in watsonx.ai. At the very top of the UI, there will be `Projects / <project name> /`. Click on the `<project name>` link. Then get the `project_id` from Project's Manage tab (Project -> Manage -> General -> Details).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "project_id = \"***\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"data\"></a>\n",
    "## Document data loading\n",
    "\n",
    "Download the file with State of the Union."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "filename = 'itr-faq.pdf'\n",
    "url = 'https://www.incometax.gov.in/iec/foportal/sites/default/files/2024-06/Top%2010%20issues%20of%20taxpayers%20updated.pdf'\n",
    "\n",
    "if not os.path.isfile(filename):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded {filename}\")\n",
    "    else:\n",
    "        print(f\"Failed to download file. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--@ 1 krishna  staff  231539 Jul 21 14:21 itr-faq.pdf\n"
     ]
    }
   ],
   "source": [
    "ls -ltr itr-faq*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"build_base\"></a>\n",
    "## Build up knowledge base\n",
    "\n",
    "The most common approach in RAG is to create dense vector representations of the knowledge base in order to calculate the semantic similarity to a given user query.\n",
    "\n",
    "In this basic example, we take the Income tax returns FAQ content (in PDF file), split it into chunks, embed it using an open-source embedding model, load it into <a href=\"https://www.trychroma.com/\" target=\"_blank\" rel=\"noopener no referrer\">Chroma</a>, and then query it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "loader = PyPDFLoader(filename)\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The dataset we are using is already split into self-contained passages that can be ingested by Chroma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create an embedding function\n",
    "\n",
    "Note that you can feed a custom embedding function to be used by chromadb. The performance of Chroma db may differ depending on the embedding model used. In following example we use watsonx.ai Embedding service. We can check available embedding models using `get_embedding_model_specs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_count': 5,\n",
       " 'limit': 100,\n",
       " 'first': {'href': 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2023-09-30&filters=function_embedding'},\n",
       " 'resources': [{'model_id': 'baai/bge-large-en-v1',\n",
       "   'label': 'bge-large-en-v1',\n",
       "   'provider': 'baai',\n",
       "   'source': 'baai',\n",
       "   'functions': [{'id': 'embedding'}],\n",
       "   'short_description': 'An embedding model with version 1.5. It has 335 million parameters and an embedding dimension of 1024.',\n",
       "   'long_description': 'This model has multi-functionality like dense retrieval, sparse retrieval, multi-vector, Multi-Linguality, and Multi-Granularity(8192 tokens)',\n",
       "   'input_tier': 'class_c1',\n",
       "   'output_tier': 'class_c1',\n",
       "   'number_params': '335m',\n",
       "   'limits': {'consumer-service-default': {'call_time': '10m0s'},\n",
       "    'consumer-user-default': {'call_time': '10m0s'},\n",
       "    'devops': {'call_time': '2m0s',\n",
       "     'max_output_tokens': 2,\n",
       "     'max_input_tokens': 20},\n",
       "    'lite': {'call_time': '5m0s'},\n",
       "    'v2-professional': {'call_time': '10m0s'},\n",
       "    'v2-standard': {'call_time': '10m0s'}},\n",
       "   'lifecycle': [{'id': 'available', 'start_date': '2024-05-16'},\n",
       "    {'id': 'withdrawn', 'start_date': '2024-08-05'}]},\n",
       "  {'model_id': 'ibm/slate-125m-english-rtrvr',\n",
       "   'label': 'slate-125m-english-rtrvr',\n",
       "   'provider': 'IBM',\n",
       "   'source': 'IBM',\n",
       "   'functions': [{'id': 'embedding'}],\n",
       "   'short_description': 'An embedding model. It has 125 million parameters and an embedding dimension of 768.',\n",
       "   'long_description': \"This model follows the standard 'sentence transformers' approach, relying on bi-encoders. It generates embeddings for various inputs such as queries, passages, or documents. The training objective is to maximize cosine similarity between two text pieces: text A (query text) and text B (passage text). This process yields sentence embeddings q and p, allowing for comparison through cosine similarity.\",\n",
       "   'input_tier': 'class_c1',\n",
       "   'output_tier': 'class_c1',\n",
       "   'number_params': '125m',\n",
       "   'limits': {'consumer-service-default': {'call_time': '10m0s'},\n",
       "    'consumer-user-default': {'call_time': '10m0s'},\n",
       "    'devops': {'call_time': '2m0s',\n",
       "     'max_output_tokens': 2,\n",
       "     'max_input_tokens': 20},\n",
       "    'lite': {'call_time': '5m0s'},\n",
       "    'v2-professional': {'call_time': '10m0s'},\n",
       "    'v2-standard': {'call_time': '10m0s'}},\n",
       "   'lifecycle': [{'id': 'available', 'start_date': '2024-04-18'}]},\n",
       "  {'model_id': 'ibm/slate-30m-english-rtrvr',\n",
       "   'label': 'slate-30m-english-rtrvr',\n",
       "   'provider': 'IBM',\n",
       "   'source': 'IBM',\n",
       "   'functions': [{'id': 'embedding'}],\n",
       "   'short_description': 'An embedding model. It has 30 million parameters and an embedding dimension of 384.',\n",
       "   'long_description': \"This model follows the standard 'sentence transformers' approach, relying on bi-encoders. It generates embeddings for various inputs such as queries, passages, or documents. The training objective is to maximize cosine similarity between two text pieces: text A (query text) and text B (passage text). This process yields sentence embeddings q and p, allowing for comparison through cosine similarity.\",\n",
       "   'input_tier': 'class_c1',\n",
       "   'output_tier': 'class_c1',\n",
       "   'number_params': '30m',\n",
       "   'limits': {'consumer-service-default': {'call_time': '10m0s'},\n",
       "    'consumer-user-default': {'call_time': '10m0s'},\n",
       "    'devops': {'call_time': '2m0s',\n",
       "     'max_output_tokens': 2,\n",
       "     'max_input_tokens': 20},\n",
       "    'lite': {'call_time': '5m0s'},\n",
       "    'v2-professional': {'call_time': '10m0s'},\n",
       "    'v2-standard': {'call_time': '10m0s'}},\n",
       "   'lifecycle': [{'id': 'available', 'start_date': '2024-04-18'}]},\n",
       "  {'model_id': 'intfloat/multilingual-e5-large',\n",
       "   'label': 'multilingual-e5-large',\n",
       "   'provider': 'intfloat',\n",
       "   'source': 'intfloat',\n",
       "   'functions': [{'id': 'embedding'}],\n",
       "   'short_description': 'An embedding model. It has 560 million parameters, has 24 layers and the embedding size is 1024.',\n",
       "   'long_description': 'This model gets continually trained on a mixture of multilingual datasets. It supports 100 languages from xlm-roberta.',\n",
       "   'input_tier': 'class_c1',\n",
       "   'output_tier': 'class_c1',\n",
       "   'number_params': '560m',\n",
       "   'limits': {'consumer-service-default': {'call_time': '10m0s'},\n",
       "    'consumer-user-default': {'call_time': '10m0s'},\n",
       "    'devops': {'call_time': '2m0s',\n",
       "     'max_output_tokens': 2,\n",
       "     'max_input_tokens': 20},\n",
       "    'lite': {'call_time': '5m0s'},\n",
       "    'v2-professional': {'call_time': '10m0s'},\n",
       "    'v2-standard': {'call_time': '10m0s'}},\n",
       "   'lifecycle': [{'id': 'available', 'start_date': '2024-05-16'}]},\n",
       "  {'model_id': 'sentence-transformers/all-minilm-l12-v2',\n",
       "   'label': 'all-minilm-l12-v2',\n",
       "   'provider': 'sentence-transformers',\n",
       "   'source': 'sentence-transformers',\n",
       "   'functions': [{'id': 'embedding'}],\n",
       "   'short_description': 'An embedding model with 128 token limit. It has 33.4 million parameters and an embedding dimension of 384.',\n",
       "   'long_description': 'This model follows sentence transformers approach, it maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.',\n",
       "   'input_tier': 'class_c1',\n",
       "   'output_tier': 'class_c1',\n",
       "   'number_params': '33.4m',\n",
       "   'limits': {'consumer-service-default': {'call_time': '10m0s'},\n",
       "    'consumer-user-default': {'call_time': '10m0s'},\n",
       "    'devops': {'call_time': '2m0s',\n",
       "     'max_output_tokens': 2,\n",
       "     'max_input_tokens': 20},\n",
       "    'lite': {'call_time': '5m0s'},\n",
       "    'v2-professional': {'call_time': '10m0s'},\n",
       "    'v2-standard': {'call_time': '10m0s'}},\n",
       "   'lifecycle': [{'id': 'available', 'start_date': '2024-05-16'}]}]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ibm_watsonx_ai.foundation_models.utils import get_embedding_model_specs\n",
    "\n",
    "get_embedding_model_specs(credentials.get('url'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------  ----  --------\n",
      "META_PROP NAME         TYPE  REQUIRED\n",
      "TRUNCATE_INPUT_TOKENS  int   N\n",
      "RETURN_OPTIONS         dict  N\n",
      "---------------------  ----  --------\n"
     ]
    }
   ],
   "source": [
    "from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames\n",
    "EmbedTextParamsMetaNames().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ibm import WatsonxEmbeddings\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import EmbeddingTypes\n",
    "\n",
    "\n",
    "embeddings = WatsonxEmbeddings(\n",
    "    model_id=EmbeddingTypes.IBM_SLATE_30M_ENG.value,\n",
    "    url=credentials[\"url\"],\n",
    "    apikey=credentials[\"apikey\"],\n",
    "    project_id=project_id,\n",
    "    params={\"TRUNCATE_INPUT_TOKENS\": 500}\n",
    "    )\n",
    "\n",
    "docsearch = Chroma.from_documents(texts, embeddings, collection_name=\"itr-docs\", persist_directory= \"/tmp/vector_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compatibility watsonx.ai Embeddings with LangChain\n",
    "\n",
    " LangChain retrievals use `embed_documents` and `embed_query` under the hood to generate embedding vectors for uploaded documents and user query respectively."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "help(WatsonxEmbeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"models\"></a>\n",
    "## Foundation Models on `watsonx.ai`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IBM watsonx foundation models are among the <a href=\"https://python.langchain.com/docs/integrations/llms/watsonxllm\" target=\"_blank\" rel=\"noopener no referrer\">list of LLM models supported by Langchain</a>. This example shows how to communicate with <a href=\"https://newsroom.ibm.com/2023-09-28-IBM-Announces-Availability-of-watsonx-Granite-Model-Series,-Client-Protections-for-IBM-watsonx-Models\" target=\"_blank\" rel=\"noopener no referrer\">Granite Model Series</a> using <a href=\"https://python.langchain.com/docs/get_started/introduction\" target=\"_blank\" rel=\"noopener no referrer\">Langchain</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Defining model\n",
    "You need to specify `model_id` that will be used for inferencing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
    "\n",
    "model_id = ModelTypes.GRANITE_13B_CHAT_V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Defining the model parameters\n",
    "We need to provide a set of model parameters that will influence the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.MAX_NEW_TOKENS: 100, # old: 100\n",
    "    GenParams.STOP_SEQUENCES: [\"<|endoftext|>\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain CustomLLM wrapper for watsonx model\n",
    "Initialize the `WatsonxLLM` class from Langchain with defined parameters and `ibm/granite-13b-chat-v2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_ibm import WatsonxLLM\n",
    "\n",
    "watsonx_granite = WatsonxLLM(\n",
    "    model_id=model_id.value,\n",
    "    url=credentials.get(\"url\"),\n",
    "    apikey=credentials.get(\"apikey\"),\n",
    "    project_id=project_id,\n",
    "    params=parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"predict\"></a>\n",
    "## Generate a retrieval-augmented response to a question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the `RetrievalQA` (question answering chain) to automate the RAG task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=watsonx_granite, chain_type=\"stuff\", retriever=docsearch.as_retriever())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Select questions\n",
    "\n",
    "Get questions from the previously loaded test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "  how to select a bank for refund\n",
      "Answer:\n",
      " \n",
      "To select a bank for refund, you need to add a bank account in your profile by following these steps:\n",
      "\n",
      "1. Go to Profile > My Bank Account > Add Bank Account.\n",
      "2. Provide correct bank details and validate the bank account.\n",
      "\n",
      "The request will be sent to the respective bank or NPCI for validation. Once validation is successful, you can nominate the bank account for refund.\n",
      "\n",
      "Note: While filing ITR, if you have a bank account\n"
     ]
    }
   ],
   "source": [
    "query = \"how to select a bank for refund\"\n",
    "print(f\"Question:\\n  {query}\")\n",
    "answer = qa.invoke(query)[\"result\"]\n",
    "print(f\"Answer:\\n {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "  What are the documentes required to register for legal heir\n",
      "Answer:\n",
      "  The documents required to register for legal heir include a copy of PAN of the deceased and copy of legal heir proof as per the norms. The legal heir proof can be any of the following:\n",
      "\n",
      "− The legal heir certificate issued by a court of law.\n",
      "− The legal heir certificate issued by the local revenue authorities.\n",
      "− The surviving family members certificate issued by the local revenue authorities.\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the documentes required to register for legal heir\"\n",
    "print(f\"Question:\\n  {query}\")\n",
    "answer = qa.invoke(query)[\"result\"]\n",
    "print(f\"Answer:\\n {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "  Hello\n",
      "Answer:\n",
      "  I'm sorry, I don't have the necessary knowledge to respond to that inquiry.\n"
     ]
    }
   ],
   "source": [
    "query = \"Hello\"\n",
    "print(f\"Question:\\n  {query}\")\n",
    "answer = qa.invoke(query)[\"result\"]\n",
    "print(f\"Answer:\\n {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"references\"></a>\n",
    "## References\n",
    "- [Using vectorized text with retrieval-augmented generation tasks](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-embedding-rag.html?context=wx&audience=wdp)\n",
    "\n",
    "- [Sample notebook](https://dataplatform.cloud.ibm.com/exchange/public/entry/view/d3a5f957-a93b-46cd-82c1-c8d37d4f62c6?context=wx?context=wx&audience=wdp)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
